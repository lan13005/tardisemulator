# Default Configuration for PyTorch Training and Inference Pipeline
# This configuration contains ALL available options for the TARDIS emulator training system

# Data Configuration
data:
  input_file: "inputs/training_input_parameter_round18.h5"
  output_file: "inputs/training_output_spectra_round18.h5"
  
  # TARDIS-specific parameters
  v_start_kms: 3000
  v0_kms: 5000
  t0_day: 5
  keep_v_outer: true
  
  # Data splitting
  limit_nsamples: 100 # load first limit_nsamples samples from the input and output hdf5 files
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  
  # DataLoader settings
  batch_size: 32
  shuffle: true
  num_workers: 0
  random_seed: 42
  
  # Preprocessing
  preprocessing:
    method: "standard"  # Options: standard, minmax, robust, none
    
  # Data augmentation (optional)
  augmentation:
    enabled: false
    noise_std: 0.01
    dropout_prob: 0.1

# Model Configuration
model:
  model_type: "mlp"  # Options: mlp, transformer, cnn, etc.
  input_dim: null  # Will be determined automatically from data
  output_dim: null  # Will be determined automatically from data
  
  # MLP-specific configuration
  hidden_dims: [512, 256, 128]
  activation: "relu"  # Options: relu, tanh, sigmoid, leaky_relu, gelu, swish
  dropout_rate: 0.1
  use_batch_norm: true
  use_bias: true

# Training Configuration
training:
  # Basic training settings
  epochs: 100
  
  # Loss function
  loss:
    type: "mse"  # Options: mse, mae, huber
    # For huber loss:
    # delta: 1.0
  
  # Optimizer settings
  optimizer:
    type: "adam"  # Options: adam, adamw, sgd, rmsprop, adagrad, adadelta
    learning_rate: 0.001
    weight_decay: 0.0
    # Adam-specific parameters
    beta1: 0.9
    beta2: 0.999
    eps: 1.0e-8
    amsgrad: false
    # For SGD:
    # momentum: 0.9
    # nesterov: true
    # For RMSprop:
    # alpha: 0.99
    # momentum: 0.0
    # centered: false
  
  # Learning rate scheduler
  scheduler:
    type: "reduce_on_plateau"  # Options: none, step, multistep, exponential, cosine, cosine_warm_restarts, reduce_on_plateau, cyclic, one_cycle, linear
    step_mode: "epoch"  # 'epoch' or 'batch'
    mode: "min"  # min for loss, max for accuracy
    factor: 0.5
    patience: 10
    threshold: 1.0e-4
    min_lr: 1.0e-6
    # For step scheduler:
    # step_size: 30
    # gamma: 0.1
    # For multistep scheduler:
    # milestones: [30, 80]
    # gamma: 0.1
    # For exponential scheduler:
    # gamma: 0.95
    # For cosine scheduler:
    # T_max: 100
    # eta_min: 1e-6
    # For cosine_warm_restarts:
    # T_0: 10
    # T_mult: 2
    # eta_min: 1e-6
    # For cyclic scheduler:
    # base_lr: 1e-4
    # max_lr: 1e-2
    # step_size_up: 2000
    # step_size_down: 2000
    # mode: 'triangular'
    # For one_cycle scheduler:
    # max_lr: 1e-2
    # total_steps: 1000
    # pct_start: 0.3
    # anneal_strategy: 'cos'
    # For linear scheduler:
    # start_factor: 1.0
    # end_factor: 0.1
    # total_iters: 100
  
  # Early stopping callback
  early_stopping:
    enabled: true
    patience: 20
    min_delta: 1.0e-6
    mode: "min"  # 'min' for metrics where lower is better, 'max' for higher is better
    restore_best_weights: true
    monitor: "val_loss"  # Metric to monitor for early stopping
  
  # Gradient clipping callback
  gradient_clipping: null  # Set to a value like 1.0 to enable, null to disable
  
  # Checkpointing callback
  checkpointing:
    enabled: true
    checkpoint_dir: "checkpoints"
    max_checkpoints: 5
    monitor: "val_loss"  # Metric to monitor for best checkpoint
    mode: "min"  # 'min' for metrics where lower is better, 'max' for higher is better
    save_best_only: true  # Only save best checkpoint, false to save all checkpoints
  
  # Logging callback
  logging:
    enabled: true
    log_dir: "logs"
    experiment_name: "default_experiment"
    use_tensorboard: true
    log_every_n_batches: 10  # Log every N batches during training
    log_level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
  
  # Diagnostic plotting callback for training curves and validation samples
  diagnostic_plotting_curves:
    enabled: true
    plot_dir: "plots"
    num_validation_samples: 5  # Number of validation samples to plot (1-10 recommended)
    update_frequency: 5  # Update plots every N epochs
    wavelength_range: [3500, 7000]  # Wavelength range for plotting (Angstroms)
    seed: 42  # Random seed for selecting validation samples to track (for reproducible sample selection)
  
  # Diagnostic plotting callback for pairwise input analysis
  diagnostic_plotting_pairplot:
    enabled: true
    plot_dir: "plots"
    update_frequency: 10  # Update plots every N epochs (less frequent due to compute cost)
    num_bins: 20  # Number of bins for X and Y axes in the histogram/contour plot
    input_scaler: null  # Scaler for inverse transforming inputs (if preprocessing was applied)

# Inference Configuration
inference:
  # Model loading
  model_path: null
  config_path: null
  
  # Preprocessing
  apply_preprocessing: true
  preprocessor_path: null
  
  # Prediction settings
  batch_size: 64
  return_probabilities: false  # Not applicable for regression
  
  # Output settings
  output_format: "csv"  # Options: csv, json, pickle
  output_dir: "predictions"

# Evaluation Configuration
evaluation:
  metrics:
    - "mse"           # Primary: (Y-Y_pred)^2 for TARDIS spectrum emulation
    - "rmse"          # Root Mean Squared Error
    - "mae"           # Mean Absolute Error
    - "mape"          # Mean Absolute Percentage Error
    - "r2"            # R-squared (coefficient of determination)
    - "explained_variance"  # Explained variance score
    - "max_error"     # Maximum error
  
  # Visualization
  plot_training_curves: true
  plot_predictions: true
  
  # Results directory
  results_dir: "results"

# Experiment Configuration
experiment:
  name: "default_experiment"
  description: "Default experiment configuration"
  tags: ["baseline", "mlp"]
  
  # Reproducibility
  random_seed: 42
  deterministic: true
  
  # Device settings
  device: "auto"  # Options: auto, cpu, cuda, cuda:0, etc.
  
  # Mixed precision training
  use_amp: false
  
  # Distributed training
  distributed: false

# Scaler configuration (for inverse transformation in plotting)
# This should be the same scaler used during data preprocessing
scaler:
  type: "standard"  # 'standard', 'minmax', 'robust'